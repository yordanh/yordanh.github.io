<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yordan Hristov</title>
    <link>https://yordanh.github.io/</link>
      <atom:link href="https://yordanh.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yordan Hristov</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 12 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yordanh.github.io/img/icon-192.png</url>
      <title>Yordan Hristov</title>
      <link>https://yordanh.github.io/</link>
    </image>
    
    <item>
      <title>Accepted paper @ ICLR 21!!</title>
      <link>https://yordanh.github.io/talk/iclr21-poster/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/talk/iclr21-poster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning from Demonstration with Weakly Supervised Disentanglement</title>
      <link>https://yordanh.github.io/publication/weak-label-lfd/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/weak-label-lfd/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Disentanglement != Axis Alignment</title>
      <link>https://yordanh.github.io/post/0_disentanglement_not_axis_alignment/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/post/0_disentanglement_not_axis_alignment/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - The bottleneck layer of a VAE might be &lt;strong&gt;disentangled, up to a rotation&lt;/strong&gt;, which might make it look entangled upon the standard visual inspection following a linear latent axis interpolation. An additional PCA step could be all one needs.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;There are multiple &lt;a href=&#34;https://openreview.net/forum?id=Sy2fzU9gl&#34;&gt;metrics&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1802.05983.pdf&#34;&gt;scores&lt;/a&gt; and &lt;a href=&#34;https://openreview.net/pdf?id=By-7dz-AZ&#34;&gt;frameworks&lt;/a&gt; in the existing literature which can be used to get a quantitative measure of &lt;em&gt;how &lt;a href=&#34;https://deepai.org/machine-learning-glossary-and-terms/disentangled-representation-learning&#34;&gt;disentangled&lt;/a&gt;&lt;/em&gt; a given vector space $\mathbf{z}$ is. More concretely, the vector spaces $\mathbf{z}$ considered in the context of achieving disentanglement are usually the bottleneck layer of a deep generative latent variable model - e.g. a VAE. When the model is trained on image data, often an additional visual qualitative evaluation is used - each of the standard basis vectors spanning $\mathbf{z}$ are linearly perturbed around the origin and the reconstructed images corresponding to the series of latent perturbations are inspected as to whether they contain isolated factors of variation - abstract notions and concepts we&#39;ll call $\mathbf{c}$.&lt;/p&gt;
&lt;p&gt;Concisely, the latent space $\mathbf{z}$ of a VAE is deemed disentangled if a set of conceptually-orthogonal notions $\mathbf{c}$ are contained as a set of orthogonal vectors in $\mathbf{z}$. For example, in the image above, the notions of object size ($c_o$) and object color ($c_1$), which are conceptually-orthogonal (independent of each other), are contained in $\mathbf{z} \in \mathbb{R}^{2}$ as a pair of orthogonal vectors. Another good and visually-supported example is this recent tweet by David Pfau below:&lt;/p&gt;
&lt;div&gt;

  &lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-dnt=&#34;true&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;Disentangling&amp;quot; is a somewhat nebulous term in ML, but it is broadly about building models that can separate out different latent factors of variation - for instance, in vision, separating translation, rotation, and changes in lighting or color that leave objects invariant. 2/n &lt;a href=&#34;https://t.co/w9viBryx9D&#34;&gt;pic.twitter.com/w9viBryx9D&lt;/a&gt;&lt;/p&gt;&amp;mdash; David Pfau (@pfau) &lt;a href=&#34;https://twitter.com/pfau/status/1275821516219985930?ref_src=twsrc%5Etfw&#34;&gt;June 24, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;/div&gt;
&lt;p&gt;While intuitive, there&#39;s a major assumption in that form of visual evaluation which has been emphasized in &lt;a href=&#34;https://arxiv.org/abs/1901.07017&#34;&gt;a paper by Watters et. al&lt;/a&gt; - the assumption that the factors of variation $\mathbf{c} = \{$x pos, y pos, size, color$\}$ &lt;em&gt;align&lt;/em&gt; with the standard basis spanning $\mathbf{z}$. By &lt;em&gt;alignment&lt;/em&gt; here we mean that there is a one-to-one correspondence between individual factors of variation and the basis vectors of the latent space. As demonstrated in the paper, and below, this is not necessarily always the case.&lt;/p&gt;
&lt;h2 id=&#34;positional-variations&#34;&gt;Positional Variations&lt;/h2&gt;
&lt;p&gt;All experiments are aimed at reproducing some of the results from the &lt;a href=&#34;https://arxiv.org/abs/1901.07017&#34;&gt;Spatial Broadcast Decoder paper by Watters et. al&lt;/a&gt;. Since we want to have neat visualisations we constrain ourselves to 2D. All the data that we use is a modification of the &lt;a href=&#34;https://github.com/deepmind/dsprites-dataset&#34;&gt;Deepmind dSprites dataset&lt;/a&gt;. In the first example we have an image of a single ellipse whose X and Y positions can vary - 32 possible values for each. The X and Y position variations are the conceptually orthogonal notions $\mathbf{c} = \{c_0, c_1\}$, implicitly contained in the image pixels.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/data_pos.jpg&#34; data-caption=&#34;Data with two factors of variation - $c_0 \equiv$ X-position (32 values, green line) and $c_1 \equiv$ Y-position (32 values, red line) of the ellipse; (top) random samples from (bottom) the uniform distribution over $\mathbf{c}$&#34;&gt;
&lt;img data-src=&#34;./img/data_pos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Data with two factors of variation - $c_0 \equiv$ X-position (32 values, green line) and $c_1 \equiv$ Y-position (32 values, red line) of the ellipse; &lt;strong&gt;(top)&lt;/strong&gt; random samples from &lt;strong&gt;(bottom)&lt;/strong&gt; the uniform distribution over $\mathbf{c}$
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The trained model is a convolutional &lt;a href=&#34;https://openreview.net/forum?id=Sy2fzU9gl&#34;&gt;$\beta$-VAE&lt;/a&gt; with a spatial broadcast decoder and size for the latent space $|\mathbf{z}|$ = 8 - example implementation &lt;a href=&#34;https://github.com/yordanh/sbd-torch&#34;&gt;here&lt;/a&gt;. The question is whether we can have any 2 out of the 8 dimensions of $\mathbf{z}$ to be equivallent to $\mathbf{c}$. This is inspected in the following manner - after training, all the images are encoded and the two dimensions with smallest average variance predictions are identified as good candidates for encoding $\mathbf{c} = \{c_0, c_1\}$, following the rationale of the original paper.&lt;/p&gt;
&lt;h3 id=&#34;beta--1&#34;&gt;$\beta$ = 1&lt;/h3&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_1.png&#34; data-caption=&#34;Example results for a disentangled but unaligned representation after training a $\beta$-VAE ($\beta$=1): (left) histogram of average predicted variances for each latent dimension; (middle) true distribution of $c_0$ and $c_1$; (right) projection of $c_0$ and $c_1$ in the subspace of $\mathbf{z}$ spanned by the two vectors with smallest predicted variance (most informative)&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example results for a &lt;strong&gt;disentangled&lt;/strong&gt; but &lt;strong&gt;unaligned&lt;/strong&gt; representation after training a $\beta$-VAE ($\beta$=1): &lt;strong&gt;(left)&lt;/strong&gt; histogram of average predicted variances for each latent dimension; &lt;strong&gt;(middle)&lt;/strong&gt; true distribution of $c_0$ and $c_1$; &lt;strong&gt;(right)&lt;/strong&gt; projection of $c_0$ and $c_1$ in the subspace of $\mathbf{z}$ spanned by the two vectors with smallest predicted variance (most informative)
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_1_interp_basis.png&#34; data-caption=&#34;Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_2$ - in the range [-2,2]; (row 1) perturbing $z_0$ corresponds to changes in both X and Y position; (row 2) perturbing $z_2$ corresponds to changes in both X and Y position;&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_1_interp_basis.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_2$ - in the range [-2,2]; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing $z_0$ corresponds to changes in both X and Y position; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing $z_2$ corresponds to changes in both X and Y position;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_1_interp_latent.png&#34; data-caption=&#34;Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in Y position; (row 2) perturbing green line corresponds to changes only in X position;&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_1_interp_latent.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing red line corresponds to changes only in Y position; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing green line corresponds to changes only in X position;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/all_beta_1.png&#34; data-caption=&#34;Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.&#34;&gt;
&lt;img data-src=&#34;./img/all_beta_1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The same experiment is repeated 10 times. It is interesting to see that even though the bottleneck layer of the VAE is consistently &lt;em&gt;disentangled&lt;/em&gt; - the square of observed values for the factors of variation is preserved as such in the latent space - it is often &lt;em&gt;unaligned&lt;/em&gt; - the square is rotated. It is worth noting that the 2D subspace of $\mathbf{z}$ representing $\mathbf{c}$ is so nicely disentangled mainly due to the inductive biases the model has - i.e. the spatial broadcast decoder (as discussed in the original paper). However, that&#39;s a topic for future post, axis alignment is what we focus on here.&lt;/p&gt;
&lt;h3 id=&#34;beta--10&#34;&gt;$\beta$ = 10&lt;/h3&gt;
&lt;p&gt;One way to account for the observed rotation of the learned representations is by tuning the coefficient of the KL divergence in the loss and enforcing more independce between the latent vectors $\mathbf{z}$. For the keen, there are examples in the literature for more principled ways of enforcing such constraints - e.g. &lt;a href=&#34;https://arxiv.org/abs/1802.04942&#34;&gt;1&lt;/a&gt; and &lt;a href=&#34;http://proceedings.mlr.press/v89/esmaeili19a/esmaeili19a.pdf&#34;&gt;2&lt;/a&gt;. As seen below, increasing $\beta$ does lead to more consistent disentanglement and alignment - perturbing the basis vectors of $\mathbf{z}$ and the latent projections of $\mathbf{c}$ contain the same conceptual information.&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_10.png&#34; data-caption=&#34;Example results for a disentangled and aligned representation after training a $\beta$-VAE ($\beta$=10); rest is the same as above.&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_10.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example results for a &lt;strong&gt;disentangled&lt;/strong&gt; and &lt;strong&gt;aligned&lt;/strong&gt; representation after training a $\beta$-VAE ($\beta$=10); rest is the same as above.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_10_interp_basis.png&#34; data-caption=&#34;Interpolating the 2 basis vectors with smallest average predicted var  - $z_1$ and $z_2$ - in the range [-2,2]; (row 1) perturbing $z_2$ corresponds to changes only in Y position; (row 2) perturbing $z_1$ corresponds to changes only in X position;&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_10_interp_basis.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the 2 basis vectors with smallest average predicted var  - $z_1$ and $z_2$ - in the range [-2,2]; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing $z_2$ corresponds to changes only in Y position; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing $z_1$ corresponds to changes only in X position;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/single_beta_10_interp_latent.png&#34; data-caption=&#34;Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in Y position; (row 2) perturbing green line corresponds to changes only in X position;&#34;&gt;
&lt;img data-src=&#34;./img/single_beta_10_interp_latent.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing red line corresponds to changes only in Y position; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing green line corresponds to changes only in X position;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/all_beta_10.png&#34; data-caption=&#34;Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.&#34;&gt;
&lt;img data-src=&#34;./img/all_beta_10.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tuning coefficients of separate terms in the optimised loss function, like $\beta$, can be very case-by-case and dataset-specific.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;color-variations&#34;&gt;Color Variations&lt;/h2&gt;
&lt;p&gt;The data used in the two experiments above contains only spatial variations for which the Spatial Broadcast Decoder was specifically designed. Out of interest we repeat the same two experiments, but this time with data that has no spatial variations, only color-based ones.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/data_color.jpg&#34; data-caption=&#34;Data with two factors of variation - $c_0 \equiv$ Object color (32 values, green line) and $c_1 \equiv$ Background color (32 values, red line); (top) random data samples from (bottom) the uniform distribution over $\mathbf{c}$&#34;&gt;
&lt;img data-src=&#34;./img/data_color.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Data with two factors of variation - $c_0 \equiv$ Object color (32 values, green line) and $c_1 \equiv$ Background color (32 values, red line); &lt;strong&gt;(top)&lt;/strong&gt; random data samples from &lt;strong&gt;(bottom)&lt;/strong&gt; the uniform distribution over $\mathbf{c}$
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In the following two experiments instead of $\beta$ we explore how does the length of training affect the learned representations.&lt;/p&gt;
&lt;h3 id=&#34;epochs--100-beta--1&#34;&gt;Epochs = 100; $\beta$ = 1&lt;/h3&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_100.png&#34; data-caption=&#34;Example results for a disentangled but unaligned representation after training a $\beta$-VAE ($\beta$=1) for 100 epochs; rest is the same as above.&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_100.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example results for a &lt;strong&gt;disentangled&lt;/strong&gt; but &lt;strong&gt;unaligned&lt;/strong&gt; representation after training a $\beta$-VAE ($\beta$=1) for 100 epochs; rest is the same as above.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_100_interp_basis.png&#34; data-caption=&#34;Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_5$ - in the range [-2,2]; (row 1) perturbing $z_0$ corresponds to changes in both background color and object color; (row 2) perturbing $z_5$ corresponds to changes in both background color and object color;&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_100_interp_basis.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_5$ - in the range [-2,2]; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing $z_0$ corresponds to changes in both background color and object color; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing $z_5$ corresponds to changes in both background color and object color;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_100_interp_latent.png&#34; data-caption=&#34;Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in background color; (row 2) perturbing green line corresponds to changes only in object color;&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_100_interp_latent.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing red line corresponds to changes only in background color; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing green line corresponds to changes only in object color;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/all_color_epoch_100.png&#34; data-caption=&#34;Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.&#34;&gt;
&lt;img data-src=&#34;./img/all_color_epoch_100.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;epochs--300-beta--1&#34;&gt;Epochs = 300; $\beta$ = 1&lt;/h3&gt;
&lt;p&gt;Instead of varying $\beta$, here we vary the number of training epochs. Nonetheless a similar phenomenon is illustrated - depending on how the model is optimised, the learned manifold $\mathbf{z}$ could be disentangled, even though it does not appear as such upon the standard visual inspection following a linear interpolation along the basis vectors.&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_300.png&#34; data-caption=&#34;Example results for a disentangled and aligned representation after training a $\beta$-VAE ($\beta$=1) for 300 epochs; rest is the same as above.&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_300.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example results for a &lt;strong&gt;disentangled&lt;/strong&gt; and &lt;strong&gt;aligned&lt;/strong&gt; representation after training a $\beta$-VAE ($\beta$=1) for 300 epochs; rest is the same as above.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_300_interp_basis.png&#34; data-caption=&#34;Interpolating the 2 basis vectors with smallest average predicted var - $z_3$ and $z_4$ - in the range [-2,2]; (row 1) perturbing $z_3$ corresponds to changes only in background color; (row 2) perturbing $z_4$ corresponds to changes only in object color;&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_300_interp_basis.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the 2 basis vectors with smallest average predicted var - $z_3$ and $z_4$ - in the range [-2,2]; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing $z_3$ corresponds to changes only in background color; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing $z_4$ corresponds to changes only in object color;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/color_epoch_300_interp_latent.png&#34; data-caption=&#34;Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in background color; (row 2) perturbing green line corresponds to changes only in object color;&#34;&gt;
&lt;img data-src=&#34;./img/color_epoch_300_interp_latent.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; &lt;strong&gt;(row 1)&lt;/strong&gt; perturbing red line corresponds to changes only in background color; &lt;strong&gt;(row 2)&lt;/strong&gt; perturbing green line corresponds to changes only in object color;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/all_color_epoch_300.png&#34; data-caption=&#34;Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.&#34;&gt;
&lt;img data-src=&#34;./img/all_color_epoch_300.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results from 10 different experiments. Variance histograms &amp;amp; latent projections of $\mathbf{c}$ are shown.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accepted paper at RSS 20 workshop in Imitation Learning</title>
      <link>https://yordanh.github.io/talk/rss20-workshop/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/talk/rss20-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Composing Diverse Policies for Temporally Extended Tasks</title>
      <link>https://yordanh.github.io/publication/ral20/</link>
      <pubDate>Mon, 20 Apr 2020 23:00:06 +0100</pubDate>
      <guid>https://yordanh.github.io/publication/ral20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Presented at the Edinburgh ML Meetup</title>
      <link>https://yordanh.github.io/talk/ml-meetup/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/talk/ml-meetup/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Best paper runner-up award at the Conference on Robot Learning 2019</title>
      <link>https://yordanh.github.io/talk/best-paper-runner-up/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/talk/best-paper-runner-up/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Disentangled Relational Representations for Explaining and Learning from Demonstration</title>
      <link>https://yordanh.github.io/publication/corl19-explain-n-repeat/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/corl19-explain-n-repeat/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid system identification using switching density networks</title>
      <link>https://yordanh.github.io/publication/corl19-sdn/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/corl19-sdn/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Using Causal Analysis to Learn Specifications from Task Demonstrations</title>
      <link>https://yordanh.github.io/publication/aamas19/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/aamas19/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://yordanh.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt; | &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;porridge &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;blueberry&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; porridge &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;blueberry&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Eating...&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;
One
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
Three
&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;{{% speaker_note %}}
&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Press &lt;span style=&#34;color:#e6db74&#34;&gt;`S`&lt;/span&gt; key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;#34;/img/boards.jpg&amp;#34; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;#34;#0000FF&amp;#34; &amp;gt;}}
{{&amp;lt; slide class=&amp;#34;my-style&amp;#34; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&#39;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h3&lt;/span&gt; {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;color&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;navy&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://spectrum.chat/academic&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DynoPlan: Combining Motion Planning and Deep Neural Network based Controllers for Safe HRL</title>
      <link>https://yordanh.github.io/publication/rldm19/</link>
      <pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/rldm19/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Interpretable Latent Spaces for Learning from Demonstration</title>
      <link>https://yordanh.github.io/publication/corl18/</link>
      <pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/corl18/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Grounding Symbols in Multi-Modal Instructions</title>
      <link>https://yordanh.github.io/publication/acl17/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://yordanh.github.io/publication/acl17/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
  </channel>
</rss>
