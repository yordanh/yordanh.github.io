[{"authors":["admin"],"categories":null,"content":"PhD student at the Robust Autonomy and Decisions group, part of the Insitute for Perception, Action and Behaviour at the University of Edinburgh. I am supervised by Dr. Subramanian Ramamoorthy and Prof. Alex Lascarides.\nMy research interests are related to the field of Human-Robot Interaction, focussing on methods that allow human experts to speed up learning for robotic agents. Naturally humans can use multiple modalities of information to teach the agents certain task-specific knowledge that is crucial for the low sample complexity of the learning process. Thus, the challenge is to bridge the high-level semantic space in which the experts reason with the low-level state/action space in which the agents operate.\n","date":1592352000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1592412692,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yordanh.guthub.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"PhD student at the Robust Autonomy and Decisions group, part of the Insitute for Perception, Action and Behaviour at the University of Edinburgh. I am supervised by Dr. Subramanian Ramamoorthy and Prof. Alex Lascarides.\nMy research interests are related to the field of Human-Robot Interaction, focussing on methods that allow human experts to speed up learning for robotic agents. Naturally humans can use multiple modalities of information to teach the agents certain task-specific knowledge that is crucial for the low sample complexity of the learning process.","tags":null,"title":"Yordan Hristov","type":"authors"},{"authors":null,"categories":null,"content":" TL;DR - The bottleneck layer of a VAE might be disentangled, up to a rotation, which might make it look entangled upon the standard visual inspection following a linear latent axis interpolation. An additional PCA step could be all one needs.\nSugggestion - read the article with a light background for better experience.\n Preface There are multiple metrics, scores and frameworks in the existing literature which can be used to get a quantitative measure of how disentangled a given vector space $\\mathbf{z}$ is. More concretely, the vector spaces $\\mathbf{z}$ considered in the context of achieving disentanglement are usually the bottleneck layer of a deep generative latent variable model - e.g. a VAE. When the model is trained on image data, often an additional visual qualitative evaluation is used - each of the standard basis vectors spanning $\\mathbf{z}$ are linearly perturbed around the origin and the reconstructed images corresponding to the series of latent perturbations are inspected as to whether they contain isolated factors of variation - abstract notions and concepts we'll call $\\mathbf{c}$.\nConcisely, the latent space $\\mathbf{z}$ of a VAE is deemed disentangled if a set of conceptually-orthogonal notions $\\mathbf{c}$ are contained as a set of orthogonal vectors in $\\mathbf{z}$. For example, in the image above, the notions of object size ($c_o$) and object color ($c_1$), which are conceptually-orthogonal (independent of each other), are contained in $\\mathbf{z} \\in \\mathbb{R}^{2}$ as a pair of orthogonal vectors. Another good and visually-supported example is this recent tweet by David Pfau below:\n \u0026quot;Disentangling\u0026quot; is a somewhat nebulous term in ML, but it is broadly about building models that can separate out different latent factors of variation - for instance, in vision, separating translation, rotation, and changes in lighting or color that leave objects invariant. 2/n pic.twitter.com/w9viBryx9D\n\u0026mdash; David Pfau (@pfau) June 24, 2020   While intuitive, there's a major assumption in that form of visual evaluation which has been emphasized in a paper by Watters et. al - the assumption that the factors of variation $\\mathbf{c}$ align with the standard basis spanning $\\mathbf{z}$. By alignment here we mean that there is a one-to-one correspondence between the factors of variation and the standard basis vectors of the latent space. As demonstrated in the paper, and below, this is not necessarily always the case.\nPositional Variations All experiments are aimed at reproducing some of the results from the Spatial Broadcast Decoder paper by Watters et. al. Since we want to have neat visualisations we constrain ourselves to 2D. All the data that we use is a modification of the Deepmind dSprites dataset. In the first example we have an image of a single ellipse whose X and Y positions can vary - 32 possible values for each. The X and Y position variations are the conceptually orthogonal notions $\\mathbf{c} = \\{c_0, c_1\\}$, implicitly contained in the image pixels.\nThe trained model is a convolutional VAE with a spatial broadcast decoder and $|\\mathbf{z}|$ = 8 - example implementation here. The question is whether we can have any 2 out of the 8 dimensions of $\\mathbf{z}$ to be equivallent to $\\mathbf{c}$. This is inspected in the following manner - after training, all the images are encoded and the two dimensions with smallest average variance predictions are identified as good candidates for encoding $\\mathbf{c} = \\{c_0, c_1\\}$, following the rationale of the original paper.\n   Data with two factors of variation - $c_0 \\equiv$ X-position (32 values, green line) and $c_1 \\equiv$ Y-position (32 values, red line) of the ellipse; (top) random data samples from (bottom) the uniform distribution over $\\mathbf{c}$   Beta = 1     Example results for a disentangled but unaligned representation after training a Beta-VAE (Beta=1): (left) histogram of average predicted variances for each latent dimension; (middle) true distribution of $c_0$ and $c_1$; (right) projection of $c_0$ and $c_1$ in the subspace of $\\mathbf{z}$ spanned by the two vectors with smallest predicted variance (most informative)      Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_2$ - in the range [-2,2]; (row 1) perturbing $z_0$ corresponds to changes in both X and Y position; (row 2) perturbing $z_2$ corresponds to changes in both X and Y position;      Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in Y position; (row 2) perturbing green line corresponds to changes only in X position;      Results from 10 different experiments. Variance histograms \u0026amp; latent projections of $\\mathbf{c}$ are shown.   Beta = 10 One way to account for the observed rotation of the learned representations is by tuning the coefficient of the KL divergence in the loss and enforcing more independce between the latent vectors $\\mathbf{z}$. For the keen, there are examples in the literature for more principled ways of enforcing such constraints - e.g. 1 and 2. As seen below, increasing $\\beta$ does lead to more consistent disentanglement and alignment - perturbing the basis vectors of $\\mathbf{z}$ and the latent projections of $\\mathbf{c}$ contain the same conceptual information.\n    Example results for a disentangled and aligned representation after training a Beta-VAE (Beta=10); rest is the same as above.      Interpolating the 2 basis vectors with smallest average predicted var - $z_1$ and $z_2$ - in the range [-2,2]; (row 1) perturbing $z_2$ corresponds to changes only in Y position; (row 2) perturbing $z_1$ corresponds to changes only in X position;      Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in Y position; (row 2) perturbing green line corresponds to changes only in X position;      Results from 10 different experiments. Variance histograms \u0026amp; latent projections of $\\mathbf{c}$ are shown.   It is worth noting that tuning loss coefficients like $\\beta$ is very case-by-case and dataset-specific.\nColor Variations The data used in the two experiments above contains only spatial variations for which the Spatial Broadcast Decoder (which we also use) was specifically designed. Out of interest we repeat the same two experiments, but this time with data that has no spatial variations, only color ones.\nData    Data with two factors of variation - $c_0 \\equiv$ Object color (32 values, green line) and $c_1 \\equiv$ Background color (32 values, red line); (top) random data samples from (bottom) the uniform distribution over $\\mathbf{c}$   Epochs = 100; Beta = 1     Example results for a disentangled but unaligned representation after training a Beta-VAE (Beta=1) for 100 epochs; rest is the same as above.      Interpolating the 2 basis vectors with smallest average predicted var - $z_0$ and $z_5$ - in the range [-2,2]; (row 1) perturbing $z_0$ corresponds to changes in both background color and object color; (row 2) perturbing $z_5$ corresponds to changes in both background color and object color;      Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in background color; (row 2) perturbing green line corresponds to changes only in object color;      Results from 10 different experiments. Variance histograms \u0026amp; latent projections of $\\mathbf{c}$ are shown.   Epochs = 300; Beta = 1 Instead of varying $\\beta$, here we vary the number of training epochs. Nonetheless a similar phenomenon is illustrated - depending on how the model is optimised, the learned manifold $\\mathbf{z}$ could be disentangled, even though it does not appear as such upon the standard visual inspection following a linear interpolation along the basis vectors.\n    Example results for a disentangled but unaligned representation after training a Beta-VAE (Beta=1) for 300 epochs; rest is the same as above.      Interpolating the 2 basis vectors with smallest average predicted var - $z_3$ and $z_4$ - in the range [-2,2]; (row 1) perturbing $z_3$ corresponds to changes only in background color; (row 2) perturbing $z_4$ corresponds to changes only in object color;      Interpolating the latent projections of $c_0$ and $c_1$ - red and green lines above; (row 1) perturbing red line corresponds to changes only in background color; (row 2) perturbing green line corresponds to changes only in object color;      Results from 10 different experiments. Variance histograms \u0026amp; latent projections of $\\mathbf{c}$ are shown.   ","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594145404,"objectID":"fe502124fcdc14ec4fde421ee4c0cab2","permalink":"https://yordanh.guthub.io/post/0_disentanglement_not_axis_alignment/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/post/0_disentanglement_not_axis_alignment/","section":"post","summary":"TL;DR - The bottleneck layer of a VAE might be disentangled, up to a rotation, which might make it look entangled upon the standard visual inspection following a linear latent axis interpolation. An additional PCA step could be all one needs.\nSugggestion - read the article with a light background for better experience.\n Preface There are multiple metrics, scores and frameworks in the existing literature which can be used to get a quantitative measure of how disentangled a given vector space $\\mathbf{z}$ is.","tags":null,"title":"Disentanglement != Axis Alignment","type":"post"},{"authors":["Yordan Hristov","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1592352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592412692,"objectID":"634d36998110c8d8e270aacad0728f59","permalink":"https://yordanh.guthub.io/publication/weak-label-lfd/","publishdate":"2020-06-17T00:00:00Z","relpermalink":"/publication/weak-label-lfd/","section":"publication","summary":"Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teaching robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of a table-top robot manipulation task performed by a PR2 robot -- that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface). The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the task and data available - see supplementary materials.","tags":["Paper"],"title":"Learning from Demonstration with Weakly Supervised Disentanglement","type":"publication"},{"authors":[],"categories":null,"content":"","date":1592179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593705222,"objectID":"d21b25e5c8d40308c06ad7264b35465e","permalink":"https://yordanh.guthub.io/talk/rss20-workshop/","publishdate":"2020-06-15T00:00:00Z","relpermalink":"/talk/rss20-workshop/","section":"talk","summary":"Advances \u0026 Challenges in Imitation Learning for Robotics","tags":[],"title":"Accepted paper at RSS 20 workshop in Imitation Learning","type":"talk"},{"authors":["Daniel Angelov","Yordan Hristov","Michael Burke","Subramanian Ramamoorthy"],"categories":[],"content":"","date":1587420006,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587651867,"objectID":"5fd19d5641cf0cb3e28da04c694e2c91","permalink":"https://yordanh.guthub.io/publication/ral20/","publishdate":"2020-01-20T23:00:06+01:00","relpermalink":"/publication/ral20/","section":"publication","summary":"Robot control policies for temporally extended and sequenced tasks are often characterized by discontinuous switches between different local dynamics. These change-points are often exploited in hierarchical motion planning to build approximate models and to facilitate the design of local, region-specific controllers.","tags":[],"title":"Composing Diverse Policies for Temporally Extended Tasks","type":"publication"},{"authors":[],"categories":null,"content":"","date":1574726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578927025,"objectID":"331a6be30e5ef7b746b3c85137e020e9","permalink":"https://yordanh.guthub.io/talk/ml-meetup/","publishdate":"2019-11-26T00:00:00Z","relpermalink":"/talk/ml-meetup/","section":"talk","summary":"Disentangled Representations - How to do Interpretable Compression with Neural Models","tags":[],"title":"Presented at the Edinburgh ML Meetup","type":"talk"},{"authors":[],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578927025,"objectID":"42d4f73fa976a0a9d3a9f1295fa6a983","permalink":"https://yordanh.guthub.io/talk/best-paper-runner-up/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/talk/best-paper-runner-up/","section":"talk","summary":"","tags":[],"title":"Best paper runner-up award at the Conference on Robot Learning 2019","type":"talk"},{"authors":["Yordan Hristov","Daniel Angelov","Michael Burke","Alex Lascarides","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"6c69a64c2b0759141b6f6287ac265c0c","permalink":"https://yordanh.guthub.io/publication/corl19-explain-n-repeat/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/corl19-explain-n-repeat/","section":"publication","summary":"Learning from demonstration is an effective method for human users to instruct desired robot behaviour. However, for most non-trivial tasks of practical interest, efficient learning from demonstration depends crucially on inductive bias in the chosen structure for rewards/costs and policies. We address the case where this inductive bias comes from an exchange with a human user. We propose a method in which a learning agent utilizes the information bottleneck layer of a high-parameter variational neural model, with auxiliary loss terms, in order to ground abstract concepts such as spatial relations. The concepts are referred to in natural language instructions and are manifested in the high-dimensional sensory input stream the agent receives from the world. We evaluate the properties of the latent space of the learned model in a photorealistic synthetic environment and particularly focus on examining its usability for downstream tasks. Additionally, through a series of controlled table-top manipulation experiments, we demonstrate that the learned manifold can be used to ground demonstrations as symbolic plans, which can then be executed on a PR2 robot.","tags":["Paper"],"title":"Disentangled Relational Representations for Explaining and Learning from Demonstration","type":"publication"},{"authors":["Michael Burke","Yordan Hristov","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1572307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"721131598fc948792f8fa8b584f3c125","permalink":"https://yordanh.guthub.io/publication/corl19-sdn/","publishdate":"2019-10-29T00:00:00Z","relpermalink":"/publication/corl19-sdn/","section":"publication","summary":"Behaviour cloning is a commonly used strategy for imitation learning and can be extremely effective in constrained domains. However, in cases where the dy- namics of an environment may be state dependent and varying, behaviour cloning places a burden on model capacity and the number of demonstrations required. This paper introduces switching density networks, which rely on a categorical reparametrisation for hybrid system identification. This results in a network com- prising a classification layer that is followed by a regression layer. We use switch- ing density networks to predict the parameters of hybrid control laws, which are toggled by a switching layer to produce different controller outputs, when condi- tioned on an input state. This work shows how switching density networks can be used for hybrid system identification in a variety of tasks, successfully identifying the key joint angle goals that make up manipulation tasks, while simultaneously learning image-based goal classifiers and regression networks that predict joint angles from images. We also show that they can cluster the phase space of an inverted pendulum, identifying the balance, spin and pump controllers required to solve this task. Switching density networks can be difficult to train, but we introduce a cross entropy regularisation loss that stabilises training.","tags":["Paper"],"title":"Hybrid system identification using switching density networks","type":"publication"},{"authors":["Daniel Angelov","Yordan Hristov","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"7ce47692bcbedebace4c6c16715c9ebd","permalink":"https://yordanh.guthub.io/publication/aamas19/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/aamas19/","section":"publication","summary":"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.","tags":["Paper"],"title":"Using Causal Analysis to Learn Specifications from Task Demonstrations","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yordanh.guthub.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Daniel Angelov","Yordan Hristov","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1540857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"e9f97cbd4039d6b408ff2a51241f5ae2","permalink":"https://yordanh.guthub.io/publication/rldm19/","publishdate":"2018-10-30T00:00:00Z","relpermalink":"/publication/rldm19/","section":"publication","summary":"Many realistic robotics tasks are best solved compositionally, through control architectures that sequentially invoke prim- itives and achieve error correction through the use of loops and conditionals taking the system back to alternative earlier states. Recent end-to-end approaches to task learning attempt to directly learn a single controller that solves an entire task, but this has been difficult for complex control tasks that would have otherwise required a diversity of local primi- tive moves, and the resulting solutions are also not easy to inspect for plan monitoring purposes. In this work, we aim to bridge the gap between hand designed and learned controllers, by representing each as an option in a hybrid hierarchical Reinforcement Learning framework - DynoPlan. We extend the options framework by adding a dynamics model and the use of a nearness-to-goal heuristic, derived from demonstrations. This translates the optimization of a hierarchical policy controller to a problem of planning with a model predictive controller. By unrolling the dynamics of each option and assessing the expected value of each future state, we can create a simple switching controller for choosing the opti- mal policy within a constrained time horizon similarly to hill climbing heuristic search. The individual dynamics model allows each option to iterate and be activated independently of the specific underlying instantiation, thus allowing for a mix of motion planning and deep neural network based primitives. We can assess the safety regions of the resulting hy- brid controller by investigating the initiation sets of the different options, and also by reasoning about the completeness and performance guarantees of the underpinning motion planners.","tags":["Paper"],"title":"DynoPlan: Combining Motion Planning and Deep Neural Network based Controllers for Safe HRL","type":"publication"},{"authors":["Yordan Hristov","Alex Lascarides","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1540857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"0b86342ad86a8d227abdd8cb3b14550a","permalink":"https://yordanh.guthub.io/publication/corl18/","publishdate":"2018-10-30T00:00:00Z","relpermalink":"/publication/corl18/","section":"publication","summary":"Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high- dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their correspond- ing sensory observations in order to align the learnt compressed latent represen- tation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.","tags":["Paper"],"title":"Interpretable Latent Spaces for Learning from Demonstration","type":"publication"},{"authors":["Yordan Hristov","Svetlin Penkov","Alex Lascarides","Subramanian Ramamoorthy"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119535,"objectID":"07b30c6dbd12c768b8af7535cf31dfb9","permalink":"https://yordanh.guthub.io/publication/acl17/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/acl17/","section":"publication","summary":"As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability---for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users' contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input---i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations---to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user's notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.","tags":["Paper"],"title":"Grounding Symbols in Multi-Modal Instructions","type":"publication"}]