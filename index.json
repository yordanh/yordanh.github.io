[{"authors":null,"categories":null,"content":"","date":1540764000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540764000,"objectID":"0b86342ad86a8d227abdd8cb3b14550a","permalink":"https://yordanh.github.io/publication/corl18/","publishdate":"2018-10-29T00:00:00+02:00","relpermalink":"/publication/corl18/","section":"publication","summary":"Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.","tags":null,"title":"Interpretable Latent Spaces for Learning from Demonstration","type":"publication"},{"authors":null,"categories":null,"content":"","date":1496264400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496264400,"objectID":"07b30c6dbd12c768b8af7535cf31dfb9","permalink":"https://yordanh.github.io/publication/acl17/","publishdate":"2017-06-01T00:00:00+03:00","relpermalink":"/publication/acl17/","section":"publication","summary":"As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability---for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users' contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input---i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations---to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user's notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.","tags":null,"title":"Grounding Symbols in Multi-Modal Instructions","type":"publication"}]